{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEyOdoVCBbGGWiy/1sjwMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nichtigVermissen/PyTorchZeroToAll/blob/master/lec_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1sl2A6drza7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBxLaS-Pw5Fo"
      },
      "source": [
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUlXllsTxFS_"
      },
      "source": [
        "w = 1.0 # a random guess: random value\n",
        "\n",
        "# our model for the foward pass\n",
        "def forward(x):\n",
        "  return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(x,y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) ** 2\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x,y):  # d_loss/d_w\n",
        "  return 2 * x * (x * w - y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgGgVMhRkNSX"
      },
      "source": [
        "# Before training\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w = w - 0.01 * grad\n",
        "    print(\"\\tgrad: \", x_val, y_val, grad)\n",
        "    l = loss(x_val, y_val)\n",
        "\n",
        "  print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
        "\n",
        "# After training\n",
        "print(\"predict (after training)\", \"4 hours\", forward(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5UCWB63muUI"
      },
      "source": [
        "# Exercise 3-1: compute gradient\n",
        "$ {\\displaystyle\\hat{y}} = x^2w_2 + xw_1 + b$\n",
        "\n",
        "$ loss = ({\\displaystyle\\hat{y}} -y) ^ 2$\n",
        "\n",
        "\n",
        "$ {\\partial loss \\over \\partial w_1} =  2x(x^2w_2 + xw_1 + b - y) $\n",
        "\n",
        "\n",
        "$ {\\partial loss \\over \\partial w_2} = 2x^2(x^2w_2 + xw_1 + b - y)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8qk-uUOu0vz"
      },
      "source": [
        "# find x^2 - 2x\n",
        "X = [1.0, 2.0, 3.0]\n",
        "Y = [3.0, 8.0, 15.0 ]\n",
        "# start with 2x^2 - x\n",
        "w1 = 1.0\n",
        "w2 = 2.0\n",
        "\n",
        "def forward(x):\n",
        "  return (x**2) * w2 + x * w1\n",
        "\n",
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) ** 2\n",
        "\n",
        "def gradient(x,y):\n",
        "  return [2 * x * ((x**2) * w2 + x * w1 - y), 2 * (x**2) * ((x**2) * w2 + x * w1 - y)]\n",
        "\n",
        "print(\"predict (before training)\", 4, forward(4))\n",
        "\n",
        "for epoch in range(200):\n",
        "  for x_val, y_val in zip(X,Y):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    w1 = w1 - 0.01 * grad[0]\n",
        "    w2 = w2 - 0.01 * grad[1]\n",
        "    l = loss(x_val, y_val)\n",
        "  print(\"progress:\", epoch, \"w1=\", w1, \"w2=\", w2, \"loss=\", l)\n",
        "\n",
        "print(\"predict (after training)\", 4, forward(4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}